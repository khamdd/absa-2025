{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e5451-85ca-4b6d-9007-927c17a2ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q torch\n",
    "!pip install -q huggingface_hub[hf_xet]\n",
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450590b5-ab8e-4b08-957c-aa0d6412cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6986d-ae18-4962-a6da-9930a395a3d8",
   "metadata": {},
   "source": [
    "# Constants Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd34603-4f11-4140-9a70-e5355e892e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'Restaurants_Train_v2.xml'\n",
    "MAX_LENGTH = 256\n",
    "PRETRAINED_MODEL = 'bert-base-uncased'\n",
    "MODEL_NAME = \"Restaurant-v1\"\n",
    "BATCH_SIZE = 21\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b3061-1a52-4460-b941-80e4bbfa4755",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182beed1-0f56-46f2-9985-c60c9707c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(TRAIN_PATH)\n",
    "root = tree.getroot()\n",
    "\n",
    "data = []\n",
    "\n",
    "for sentence in root.iter(\"sentence\"):\n",
    "    text = sentence.find(\"text\").text\n",
    "    aspect_categories = sentence.find(\"aspectCategories\")\n",
    "    \n",
    "    if aspect_categories is not None:\n",
    "        for category in aspect_categories.findall(\"aspectCategory\"):\n",
    "            cat = category.attrib[\"category\"]\n",
    "            polarity = category.attrib[\"polarity\"]\n",
    "            if polarity != \"conflict\":\n",
    "                data.append({\n",
    "                    \"sentence\": text,\n",
    "                    \"category\": cat,\n",
    "                    \"label\": polarity\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['label'] = df['label'].map(label_map)\n",
    "df.to_csv(\"absa_dataset.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd915a-736f-4f8c-9ea3-88a62063e5e6",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c3402-ce05-4024-b030-46b1340fd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['sentence'].tolist()\n",
    "categories = df['category'].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    sentences, categories, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93844c-83a3-49fa-aa59-6857eacb1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bbb2a3-3656-413c-ad24-0e544c6aff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5590972-c86c-44d6-a009-0ffe3d7ec0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train + y_val + y_test)\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "train_dataset = Dataset(train_encodings, y_train_encoded)\n",
    "val_dataset = Dataset(val_encodings, y_val_encoded)\n",
    "test_dataset = Dataset(test_encodings, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d7db7-2a75-47d0-ae38-2eecc42d4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5b396-7a0c-4aff-aad9-fc3683a75928",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cb4438d6-b020-4065-967d-340386eeb2ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m     PRETRAINED_MODEL,\n\u001b[0;32m      5\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4333\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4330\u001b[0m config\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m   4332\u001b[0m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[1;32m-> 4333\u001b[0m model_init_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_init_context(is_quantized, _is_ds_init_called)\n\u001b[0;32m   4335\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m   4336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3736\u001b[0m, in \u001b[0;36mPreTrainedModel.get_init_context\u001b[1;34m(cls, is_quantized, _is_ds_init_called)\u001b[0m\n\u001b[0;32m   3734\u001b[0m         init_contexts\u001b[38;5;241m.\u001b[39mappend(set_quantized_state())\n\u001b[0;32m   3735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3736\u001b[0m     init_contexts \u001b[38;5;241m=\u001b[39m [no_init_weights(), init_empty_weights()]\n\u001b[0;32m   3738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m init_contexts\n",
      "\u001b[1;31mNameError\u001b[0m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    num_labels=3,\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a4109-d75b-491c-a87e-9597f9bca7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (for classification tasks)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (using AdamW, which is recommended for BERT)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e47df-06cf-4e93-8f68-876de67c188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move tensors to the appropriate device (GPU/CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {running_loss / len(train_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
