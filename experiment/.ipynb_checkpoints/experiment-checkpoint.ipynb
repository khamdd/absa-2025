{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927e5451-85ca-4b6d-9007-927c17a2ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q datasets\n",
    "!pip install -q accelerate\n",
    "!pip install -q transformers\n",
    "!pip install -q emoji\n",
    "!pip install -q tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450590b5-ab8e-4b08-957c-aa0d6412cdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.19.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be075d-8012-400f-883d-80f707301099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will result in error, ignore it\n",
    "\n",
    "# if tf.config.list_physical_devices('GPU'):\n",
    "#     physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#     print('Using GPU:', tf.test.gpu_device_name())\n",
    "#     !nvcc -V\n",
    "# else: raise ValueError('Running on CPU is not recommended.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this step if run locally\n",
    "\n",
    "# !git clone https://github.com/khamdd/absa-2025\n",
    "# %cd ./absa-2025\n",
    "# !mkdir predictions\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6986d-ae18-4962-a6da-9930a395a3d8",
   "metadata": {},
   "source": [
    "# Constants Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd34603-4f11-4140-9a70-e5355e892e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_PATH = r'../datasets/eval_2014/Restaurants_Train_v2.xml'\n",
    "TRAIN_PATH = r'../datasets/eval_2014/Restaurants_Train_v2_train.csv'\n",
    "VAL_PATH = r'../datasets/eval_2014/Restaurants_Train_v2_dev.csv'\n",
    "TEST_PATH = r'../datasets/eval_2014/Restaurants_Train_v2_test.csv'\n",
    "PRETRAINED_MODEL = 'bert-base-uncased'\n",
    "MODEL_NAME = \"Restaurant-v1\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 21\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b3254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file generated:../datasets/eval_2014/Restaurants_Train_v2_train.csv\n"
     ]
    }
   ],
   "source": [
    "from processors.eval2014_processor import Eval2014Loader\n",
    "\n",
    "Eval2014Loader.xmlToCSV(RAW_DATASET_PATH, TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f02df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2128\n",
      "Validation size: 456\n",
      "Test size: 457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (assuming it's a CSV)\n",
    "dataset = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "# Define the split ratios\n",
    "train_size = 0.7  # 70% for training\n",
    "test_val_size = 0.3  # Remaining 30% for testing and validation\n",
    "val_size = 0.5  # 50% of the test_val set for validation\n",
    "\n",
    "# Step 1: Split into train and remaining (test + validation)\n",
    "train_data, test_val_data = train_test_split(dataset, test_size=test_val_size, random_state=42)\n",
    "\n",
    "# Step 2: Split the remaining data into test and validation\n",
    "val_data, test_data = train_test_split(test_val_data, test_size=val_size, random_state=42)\n",
    "\n",
    "# Verify the sizes\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "# Save the splits into separate files (optional)\n",
    "train_data.to_csv(TRAIN_PATH, index=False)\n",
    "val_data.to_csv(VAL_PATH, index=False)\n",
    "test_data.to_csv(TEST_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e311bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = Eval2014Loader.load(TRAIN_PATH, VAL_PATH, TEST_PATH)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89ab0c",
   "metadata": {},
   "source": [
    "# Preprocess and Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92feeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processors.english_processor import EnglishTextPreprocessor\n",
    "\n",
    "eng_preprocessor = EnglishTextPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "tokens = tokenizer.encode(\"This is a auto tokenizer test string\")\n",
    "print('Encode:', tokens, '\\nDecode:', tokenizer.decode(tokens))\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd039a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_datasets = Eval2014Loader.preprocess_and_tokenize(raw_datasets, eng_preprocessor, tokenizer, BATCH_SIZE * 2, MAX_LENGTH)\n",
    "preprocessed_datasets.save_to_disk('../datasets/preprocessed_restaurant')\n",
    "display(preprocessed_datasets)\n",
    "pd.DataFrame({\n",
    "    'raw_datasets': raw_datasets['train']['Review'][:10],\n",
    "    'encoded_input_ids': preprocessed_datasets['train']['input_ids'][:10],\n",
    "    'decoded_input_ids': [tokenizer.decode(preprocessed_datasets['train'][i]['input_ids']) for i in range(10)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231bd61-527b-404a-a53d-54407d17c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "preprocessed_datasets = load_from_disk('../datasets/preprocessed_restaurant')\n",
    "preprocessed_datasets = Eval2014Loader.labels_to_flatten_onehot(preprocessed_datasets)\n",
    "preprocessed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34bb2c-62e0-4f12-9796-09c18519cca1",
   "metadata": {},
   "source": [
    "# Prepare for TensorFlow Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09368bf1-0769-4f81-9139-e9b23dbc9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORY_NAMES = raw_datasets['train'].column_names[1:]\n",
    "steps_per_epoch = len(preprocessed_datasets['train']) // BATCH_SIZE\n",
    "total_steps = EPOCHS * steps_per_epoch\n",
    "print(ASPECT_CATEGORY_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe12a6-1467-4a61-86a3-ffe382e652d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset = preprocessed_datasets['train'].to_tf_dataset(\n",
    "    columns=tokenizer.model_input_names, label_cols='FlattenOneHotLabels',\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b3159-1217-4d2f-a4c3-66d394c6edb3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tf_keras.optimizers import Adam\n",
    "from tf_keras.optimizers.schedules import CosineDecay\n",
    "from tf_keras.callbacks import EarlyStopping\n",
    "\n",
    "optimizer = Adam(learning_rate=CosineDecay(\n",
    "    initial_learning_rate = 1e-4,\n",
    "    warmup_target = 2e-4,\n",
    "    warmup_steps = int(total_steps * 0.15), # 15% of total_steps\n",
    "    decay_steps = int(total_steps * 0.3), # Next 30% of total_steps\n",
    "    alpha = 0.1, # Minimum lr for decay as a fraction of initial_learning_rate\n",
    "))\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3, # Stop if no improvement after 3 epochs\n",
    "    restore_best_weights = True, # You can obtain HIGHER metrics on the test set with longer training time if you set this to False\n",
    "    # Because after some experiments, I found that even with higher val_loss, it still results in better metric reports on the test set. \n",
    "    # This maybe because the training set and the test set have more similarities than the validation data.\n",
    "    # But I think this is not fair, as we already have prior knowledge about the test set and we modified our training based on the performance on this set. \n",
    "    # In real-world, we should only modify our training based on the performance on the validation data\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5b396-7a0c-4aff-aad9-fc3683a75928",
   "metadata": {},
   "source": [
    "# Fine Tuning with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990850e-c2da-4256-bbdf-ba61cd0a886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from acsa_model import ABSA2025MultiTask\n",
    "from helper import plot_training_history\n",
    "model = ABSA2025MultiTask(PRETRAINED_MODEL, ASPECT_CATEGORY_NAMES, optimizer, name=MODEL_NAME)\n",
    "\n",
    "history = model.fit(\n",
    "    train_tf_dataset,\n",
    "    # validation_data = val_tf_dataset,\n",
    "    callbacks = [early_stop_callback],\n",
    "    epochs = EPOCHS,\n",
    "    verbose = 1\n",
    ").history\n",
    "\n",
    "model.save_weights(f'./weights/{MODEL_NAME}/{MODEL_NAME}', save_format='tf')\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
